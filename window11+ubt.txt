1.Change image sources

sudo vim /etc/apt/sources.list

deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse


2. 在Windows11上安装ubuntu版linux系统并实现桌面图形化（WSL）
https://blog.csdn.net/zjjlov/article/details/125900597



# 我也使用的wget ,我下载到了home中
wget https://www.python.org/ftp/python/3.9.0/Python-3.9.0.tgz
# 在home中解压
tar -zxf Python-3.9.0.tgz
# 进入python3.9
cd Python-3.9.0
 
# 编译文件  时间大概有1-3分钟
./configure --prefix=/usr/local/python3
 
# 编译好后，会有另外一个提示，让run ./configure xxx
./configure --enable-optimizations
 
# 安装
make && make install
 
# 结束

1、查找到lsb_release模块所在的目录
sudo find / -name 'lsb_release.py'
 
2、将其复制到设置python3.8的系统模块加载位置，也就是报错处subprocess.py所在的目录
sudo cp  /usr/lib/python3/dist-packages/lsb_release.py /usr/local/python3/lib/python3.9/


sudo apt-get update
sudo apt-get upgrade


sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32


ln -s ./python ./python2.7*

ln -s ./python3.6* ./python3

pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}" -i http://mirrors.aliyun.com/pypi/simple/

阿里云 http://mirrors.aliyun.com/pypi/simple/

中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/

豆瓣(douban) http://pypi.douban.com/simple/

清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/

中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/

pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}" -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com

3. 
ssh ip172-18-0-55-csio0u291nsg00ci9q40@direct.labs.play-with-docker.com

4.

docker-compose up -d

docker-compose down

airflow:
 source /usr/bin/airflow_python/bin/activate
 deactivate
home: /opt/airflow

启动:
airflow standalone

关闭：master


 admin  password: 2aYregPnzFCsh69k


airflow users create \
    --username admin \
    --firstname Peter \
    --lastname Parker \
    --role Admin \
    --email spiderman@superhero.org

5.grafana
podman run -d -p 3000:3000 --name=grafana docker.io/grafana/grafana


6.Loki
podman run  -p 3100:3100 --name loki docker.io/grafana/loki v /opt/grafana/loki:/etc/loki v /opt/grafana/loki/loki_config.yaml:/etc/loki/loki_config.yaml -config.file=/etc/loki/loki_config.yaml


server:
  http_listen_port: 3100

schema_config:
  configs:
    - from: 2024-07-01
      store: boltdb
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s

storage_config:
  boltdb:
    directory: /opt/grafana/promtail/index
  filesystem:
    directory: /opt/grafana/promtail/chunks

limits_config:
  enforce_metric_name: true
  reject_old_samples: true
  reject_old_samples_max_age: 168h
  ingestion_rate_mb: 32
  ingestion_burst_size_mb: 64

chunk_store_config:
  max_look_back_period: 672h


7.promtail
podman run -d -p 3100:3100 --name=promtail docker.io/grafana/promtail -config.file=/etc/promtail/promtail_config.yaml -v /opt/grafana/promtail:/etc/promtail -v /opt/airflow/logs:var/airflow_logs --network host

server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://127.0.0.1:3100/loki/api/v1/push

scrape_configs:
  - job_name: service_logs
    static_configs:
      - targets:
          - 127.0.0.1
        labels:
          job: dag_1
          __path__: var/airflow_logs/dag_id=example_display*/*/*/*.log
      - targets:
          - 127.0.0.1
        labels:
          job: dag_2
          __path__: var/airflow_logs/dag_id=wx_time_*/*/*/*.log
    pipeline_stages:
      - match:
          selector: '{job="dag_1"} |= "group"'
          stages:
            - multiline:
                firstline: '^\[\d{4}-\d{2}-\d{2}'
                max_wait_time: 3s
            - regex:
                expression: '^\[(?P<times>[-T\d+:.]{28}).*'
            - timestamp:
                format: 2006-01-02T15:04:05-0700
                source: times
                location: Asia/Shanghai
            - labels:
                times:
      - match:
          selector: '{job="dag_2"}'
          stages:
            - multiline:
                firstline: '^\[\d{4}-\d{2}-\d{2}'
                max_wait_time: 3s
            - regex:
                expression: '^\[(?P<times>[-T\d+:.]{28}).*'
            - timestamp:
                format: 2006-01-02T15:04:05-0700
                source: times
                location: Asia/Shanghai
            - labels:
                times:


# 其中username为ubuntu子系统的用户名
# path 为当前文件所在的目录
sudo chown -R root /opt/airflow






99. 打包docker image
# 创建pod
 podman pod create --name mypod
# 将停用的mysql打包为本地镜像
docker commit redis redisimage
# 1. 保存 Docker 镜像
docker save -o mysql.tar mysql:8.0

# 2. 将 tar 文件导入到 Podman
podman load -i mydockerimage.tar

# 3. 验证镜像
podman images

# 5. 运行容器并将其加入到 Pod 中
odman run -d --name podredis --pod mypod mysql:8.0

